# RLOO

# Секция экспериментов

## 1. Mean baseline

  ![image](https://github.com/user-attachments/assets/39de1144-1217-4716-9c2d-fb9f4eb1bb94)

- **Кривая обучения:**  
  В первые ≈100 эпизодов агент улучшает политику очень медленно (возвраты в районе 20–30). Значимый рост начинается лишь после эпизода 150, и к концу достигает 90–100.

- **Средняя оценка эксперта:**  
  `mean_expert`: **277.40** шагов.
  

### Преимущества
- Простейшая реализация: нет необходимости в дополнительной сети.

### Недостатки
- Бейзлайн не учитывает текущее состояние → слабая адаптивность.
- Очень медленное и нестабильное обучение, чувствительность к масштабу вознаграждений.

---

## 2. Value baseline 

![image](https://github.com/user-attachments/assets/ac5de789-219c-46dc-8ff2-e92553c1e657)


- **Кривая обучения:**  
  Агент начинает получать первые  улучшения уже примерно к 30 эпизоду. К середине тренировки средние возвраты достигают 50–100, затем подрастают к 200, но сохраняют значительный разброс.

- **Средняя оценка эксперта:**  
  `value_expert`: **238.20** шагов.

### Преимущества
- State-dependent бейзлайн (value-сеть) сильнее центрирует возвраты - ещё меньшая дисперсия градиента.
- Быстрый начальный рост качества по сравнению с константным бейзлайном.

### Недостатки
- Появляется дополнительная нейросеть (критик), требующая отдельного обновления и аккуратной настройки скорости обучения.

---

## 3. RLOO baseline

![image](https://github.com/user-attachments/assets/2588c33b-c2d4-4325-849d-1a626163e473)

- **Кривая обучения:**  
  Уже к 50–60 эпизодам возвращения выходят за 100, далее почти линейно растут и стабильно достигают максимума среды (500) к концу обучения.

- **Средняя оценка эксперта:**  
  `rloo_expert`: **500.00** шагов (идеальное решение).

### Преимущества
- бейзлайн полностью устраняет влияние текущего эпизода на оценку - минимальная дисперсия
- быстрая сходимость и стабильно максимальное качество

### Недостатки
- ХЗ, кроме более сложной реализации 

---

## Сводная таблица результатов

| Бейзлайн | Средняя награда по 200 эпизодам |
|:--------:|:------------------------------:|
| mean     | 277.40                         |
| value    | 238.20                         |
| rloo     | 500.00                         |

---

## Выводы

1. **Mean baseline** легко реализовать, но он даёт мало адаптивности и медленную сходимость.  
2. **Value baseline** ускоряет начало обучения за счёт  оценки, но требует дополнительного критика и тщательной настройки.  
3. **RLOO baseline** сочетает в себе простоту константного подхода и низкую дисперсию: обеспечивает быструю и стабильную сходимость к оптимуму, но чуть более сложен в реализации

---

# Behaviour Cloning


## 1. Сбор демонстраций эксперта
- **Эксперт**: лучший результат получили с исопльзованием RLOO-style бейзлайн.  
- **Рандомизация стартов**: при каждом `env.reset()` мы дополнительно смещаем угол штанги в пределах ±0.05 рад, чтобы покрыть разные точки в окрестности нуля.  
- **Фильтрация**: в датасет попадают только эпизоды, где эксперт удерживал штангу всю максимальную длину (`max_steps`), гарантируя, что все пары `(состояние→действие)` — оптимальны.  

## 2. Обучение клона (Behavior Cloning)
- Использовали ту же архитектуру PolicyNet.  
- Обучали на датасете из 2000 шагов экспертных траекторий. 10 эпох lr=1e-3.  

## 3. Оценка качества
- **На обычных эпизодах** :  
  - Эксперт показал результат лучше, точно можно было поулчить одинаковые результаты, если учить на большем количестве эпизодов.  
- **На стартах с большим углом**:  
  - Эксперт сохраняет способность балансировать, клон же ещё сильнее потерял в качестве.

## 4. Выводы и дизайн экспериментов
1. **BC хорошо** работает в диапазоне состояний, встреченных в демонстрациях.  
2. **BC плохо** масштабируется на более экзотичные старыт: модель не знает, какое действие следует предпринимать.  
